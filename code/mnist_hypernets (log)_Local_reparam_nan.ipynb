{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import json\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # cuda or cpu\n",
    "device = t.device(device)\n",
    "if device == 'cuda':\n",
    "    t.backends.cudnn.deterministic = True\n",
    "    t.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "init_log_sigma = -3.0 # логарифм дисперсии вариационного распределения при инициализации\n",
    "prior_sigma = 0.1 # априорная дисперсия\n",
    "epoch_num = 25 #количество эпох\n",
    "lamb = [0.1, 1,  10, 100]\n",
    "hidden_num = 100 #количество нейронов на скрытом слое\n",
    "acc_delete = [] \n",
    "start_num = 5\n",
    "\n",
    "\n",
    "lam_hidden_num = 50\n",
    "lambda_sample_num = 5\n",
    "path_to_save = 'saved'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_to_save):\n",
    "    os.mkdir(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "train_data = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "test_data = torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True )\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLayer(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = nn.Parameter(t.randn(in_, out_, device=device)) # параметры средних\n",
    "        t.nn.init.xavier_uniform(self.mean) \n",
    "        self.log_sigma = nn.Parameter(t.ones(in_, out_, device = device)*init_log_sigma) # логарифм дисперсии\n",
    "        self.mean_b = nn.Parameter(t.randn(out_, device=device)) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = nn.Parameter(t.ones(out_, device=device) * init_log_sigma)\n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            W = self.mean\n",
    "            mu = x.matmul(W)\n",
    "            log_alpha = self.log_sigma\n",
    "            eps = 1e-8\n",
    "            si = t.sqrt((x * x).matmul(((t.exp(2*log_alpha+eps)))))\n",
    "            activation = mu + t.normal(t.zeros_like(mu), t.ones_like(mu)) * si + \\\n",
    "                t.exp(2*self.log_sigma_b) * t.normal(t.zeros_like(mu), t.ones_like(mu))   \n",
    "        \n",
    "            return self.act(activation + self.mean_b)\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean \n",
    "            b = self.mean_b\n",
    "            # функция активации \n",
    "            \n",
    "            return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b,  t.exp(self.log_sigma_b))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankNet(nn.Module):\n",
    "    def __init__(self, size, hidden, gain_const = 1.0, gain_lamb = 1.0, gain_lowrank = .0001,  act= F.relu): \n",
    "        nn.Module.__init__(self)        \n",
    "        self.w = nn.Linear(1, hidden).to(device)\n",
    "        t.nn.init.xavier_uniform(self.w.weight, gain_lamb)\n",
    "        # проверка на вектор или матрица\n",
    "        if isinstance(size, tuple) and len(size) == 2: # если сайз неизменяемый список и его длина 2\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1,device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            self.w_d = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform_(self.w_d.weight, gain_lowrank)\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "\n",
    "            \n",
    "        else:\n",
    "            self.w_a1 = nn.Linear(hidden, self.in_).to(device)\n",
    "            t.nn.init.xavier_uniform_(self.w_a1.weight, gain_lowrank)\n",
    "            \n",
    "            self.w_a2 = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform_(self.w_a2.weight, gain_lowrank)\n",
    "            \n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform_(self.const,  gain_const)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self, lam):\n",
    "        h = self.act(self.w(self.one * lam))        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.w_d(h)\n",
    "        else:\n",
    "            a1 = self.w_a1(h)\n",
    "            a2 = self.w_a2(h)\n",
    "         \n",
    "            return self.const +  t.matmul(a1.view(-1, 1), a2.view(1, -1)) \n",
    "\n",
    "        \n",
    "class LinearApprNet(nn.Module):\n",
    "    def __init__(self, size,  gain_const = 1.0, gain_const2 = 1.0,  act= lambda x: x):    \n",
    "        nn.Module.__init__(self)        \n",
    "        if isinstance(size, tuple) and len(size) == 2:\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1, device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "            self.const2 = nn.Parameter(t.ones(self.out_, device=device) * gain_const2) \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform_(self.const,  gain_const)\n",
    "            self.const2 = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform_(self.const2,  gain_const2)\n",
    "            \n",
    "            \n",
    "    def forward(self, lam):        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.const2 * lam\n",
    "        else:\n",
    "            return self.const + self.const2 * lam \n",
    "\n",
    "        \n",
    "class VarLayerLowRank(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LowRankNet((in_, out_), lam_hidden_num) # параметры средних            \n",
    "        self.log_sigma = LowRankNet((in_, out_), lam_hidden_num) # логарифм дисперсии\n",
    "        self.mean_b = LowRankNet( out_, lam_hidden_num) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LowRankNet( out_, lam_hidden_num)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x, l):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            W = self.mean(l)\n",
    "            mu = x.matmul(W)\n",
    "            log_alpha = self.log_sigma(l)\n",
    "            eps = 1e-6\n",
    "            if (t.isinf(2*log_alpha +eps)).any():\n",
    "                print('inf')\n",
    "            else:\n",
    "                print(t.min(log_alpha))\n",
    "            si = t.sqrt((x * x).matmul(((t.exp(2*log_alpha+eps)))))\n",
    "            activation = mu + t.normal(t.zeros_like(mu), t.ones_like(mu)) * si + \\\n",
    "                t.exp(2*self.log_sigma_b(l)) * t.normal(t.zeros_like(mu), t.ones_like(mu))   \n",
    "        \n",
    "            return self.act(activation + self.mean_b(l))\n",
    "\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "            # функция активации \n",
    "            return self.act(t.matmul(x, w)+b)\n",
    "            \n",
    "            \n",
    "\n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    \n",
    "\n",
    "class VarLayerLinearAppr(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LinearApprNet((in_, out_)) # параметры средних            \n",
    "        self.log_sigma = LinearApprNet((in_, out_)) # логарифм дисперсии\n",
    "        self.mean_b = LinearApprNet( out_) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LinearApprNet( out_)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        \n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            W = self.mean(l)\n",
    "            mu = x.matmul(W)\n",
    "            log_alpha = self.log_sigma(l)\n",
    "            eps = 1e-8\n",
    "            si = t.sqrt((x * x).matmul(((t.exp(2*log_alpha+eps)))))\n",
    "            activation = mu + t.normal(t.zeros_like(mu), t.ones_like(mu)) * si + \\\n",
    "                t.exp(2*self.log_sigma_b(l)) * t.normal(t.zeros_like(mu), t.ones_like(mu))   \n",
    "        \n",
    "            return self.act(activation + self.mean_b(l))\n",
    "\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "            # функция активации \n",
    "            return self.act(t.matmul(x, w)+b)\n",
    "    \n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarSeqNet(nn.Sequential):    \n",
    "    # класс-обертка на случай, если у нас многослойная нейронная сеть\n",
    "    def KLD(self, lam = None):\n",
    "        k = 0\n",
    "        for l in self: \n",
    "            if lam is None:\n",
    "                k+=l.KLD()\n",
    "            else:\n",
    "                k+=l.KLD(lam)\n",
    "                \n",
    "        return k\n",
    "    \n",
    "    def forward(self, x, lam = None):\n",
    "        if lam is None:\n",
    "            for l in self:\n",
    "                x = l(x)\n",
    "            return x\n",
    "        else:\n",
    "            for l in self:\n",
    "                x = l(x, lam)\n",
    "            return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(net, loss_fn, optimizer, lam, label):\n",
    "    tq = tqdm.tqdm(train_loader)\n",
    "    losses = []\n",
    "    for x,y in tq:            \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)          \n",
    "        optimizer.zero_grad()  \n",
    "        loss = 0\n",
    "        if lam is None:\n",
    "            for _ in range(lambda_sample_num):  \n",
    "                p = t.rand(1).to(device)*3 -1\n",
    "                lam_param = 10**p[0]\n",
    "                \n",
    "                #t.rand(1).to(device)[0]*100.0                  \n",
    "                out = net(x, t.log(lam_param))\n",
    "                loss = loss + loss_fn(out, y)/lambda_sample_num\n",
    "                loss += net.KLD(lam_param)*(lam_param)/len(train_data)/lambda_sample_num\n",
    "                losses+=[loss.cpu().detach().numpy()]\n",
    "            # правдоподобие должно суммироваться по всей обучающей выборке\n",
    "            # в случае батчей - она приводится к тому же порядку \n",
    "        else:\n",
    "            out = net(x)\n",
    "            loss = loss + loss_fn(out, y)\n",
    "            loss += net.KLD()*lam/len(train_data)\n",
    "            losses+=[loss.cpu().detach().numpy()]\n",
    "        tq.set_description(label+str(np.mean(losses)))\n",
    "        loss.backward()       \n",
    "        clip_grad_value_(net.parameters(), 1.0) # для стабильности градиента. С этим можно играться\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magistrkoljan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"\n",
      "lambda 0.1, epoch 0: 2.024342:  22%|██▏       | 51/235 [00:03<00:11, 16.35it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b1f90f1be42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lambda {}, epoch {}: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'var_net_lam_{}_start_{}.cpk'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-644629c09a51>\u001b[0m in \u001b[0;36mtrain_batches\u001b[0;34m(net, loss_fn, optimizer, lam, label)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "for lam in lamb:\n",
    "    for start in range(start_num):                    \n",
    "            net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "            net = net.to(device)\n",
    "            optim = t.optim.Adam(net.parameters(), lr=5e-4)\n",
    "            loss_fn = nn.CrossEntropyLoss().to(device)            \n",
    "            for e in range(epoch_num):\n",
    "                label = 'lambda {}, epoch {}: '.format(lam, e)                \n",
    "                train_batches(net, loss_fn, optim, lam, label)\n",
    "            t.save(net.state_dict(), os.path.join(path_to_save, 'var_net_lam_{}_start_{}.cpk'.format(lam, start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "linear, epoch 0: nan:  11%|█         | 25/235 [00:02<00:19, 10.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fea4f52345c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}, epoch {}: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}_start_{}.cpk'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-644629c09a51>\u001b[0m in \u001b[0;36mtrain_batches\u001b[0;34m(net, loss_fn, optimizer, lam, label)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlambda_sample_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlambda_sample_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# правдоподобие должно суммироваться по всей обучающей выборке\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-dc82c96eb8d6>\u001b[0m in \u001b[0;36mKLD\u001b[0;34m(self, lam)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-adfaaf6a1916>\u001b[0m in \u001b[0;36mKLD\u001b[0;34m(self, l)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprior_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/distributions/kl.py\u001b[0m in \u001b[0;36mkl_divergence\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/distributions/kl.py\u001b[0m in \u001b[0;36m_kl_normal_normal\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_kl_normal_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mvar_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar_ratio\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvar_ratio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for mode in ['linear']:\n",
    "    t.manual_seed(0)\n",
    "    for start in range(start_num): \n",
    "        if mode == 'lowrank':\n",
    "            net = VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x))\n",
    "        else:\n",
    "            net = VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x))\n",
    "            \n",
    "        net = net.to(device)\n",
    "        optim = t.optim.Adam(net.parameters(), lr=5e-8)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)            \n",
    "        for e in range(epoch_num):\n",
    "            label = '{}, epoch {}: '.format(mode, e)                \n",
    "            train_batches(net, loss_fn, optim, None, label)\n",
    "        t.save(net.state_dict(), os.path.join(path_to_save, '{}_start_{}.cpk'.format(mode, start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(net): # точность классификации\n",
    "    acc = []    \n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    for x,y in test_loader: \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)  \n",
    "        out = net(x)    \n",
    "        correct += out.argmax(1).eq(y).sum().cpu().numpy()\n",
    "    acc = (correct / len(test_data))\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# будем удалять по 10% от модели и смотреть качество\n",
    "def delete_10(net):\n",
    "    acc_delete = []\n",
    "    mu = net[0].mean\n",
    "    sigma = t.exp(2*net[0].log_sigma)\n",
    "    prune_coef = (mu**2/sigma).cpu().detach().numpy()    \n",
    "    sorted_coefs = np.sort(prune_coef.flatten())\n",
    "    mu2 = net[1].mean\n",
    "    sigma2 = t.exp(2*net[1].log_sigma)\n",
    "    prune_coef2 = (mu2**2/sigma2).cpu().detach().numpy()    \n",
    "    sorted_coefs2 = np.sort(prune_coef2.flatten())\n",
    "    \n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        ids = (prune_coef <= sorted_coefs[round(j/10*len(sorted_coefs))]) \n",
    "        net[0].mean.data*=(1-t.tensor(ids*1.0, device=device, dtype=t.float))\n",
    "        \n",
    "        ids2 = (prune_coef2 <= sorted_coefs2[round(j/10*len(sorted_coefs2))]) \n",
    "        net[1].mean.data*=(1-t.tensor(ids2*1.0, device=device, dtype=t.float))\n",
    "        \n",
    "        print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "        acc_delete.append(test_acc(net))\n",
    "    return acc_delete    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_results = {}\n",
    "print(type(lam_results))\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        print (lam, s)\n",
    "        net.load_state_dict(t.load(os.path.join('saved/', 'var_net_lam_{}_start_{}.cpk'.format(lam, s))))\n",
    "        lam_results[lam].append(delete_10(net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_var.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_results = {}\n",
    "with open('results_var.json','r') as out:\n",
    "    lam_results = json.loads(out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "ls = {\n",
    "    '0.1':'-',\n",
    "    '1':'--',\n",
    "    '10':':',\n",
    "    '100':'-.'\n",
    "}\n",
    "lamb = [0.1, 1,  10, 100]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "for lam in lamb:\n",
    "    lam = str(lam)\n",
    "    print(lam)\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "    #plt.errorbar(proc, np.mean(lam_results[lam], 0), \n",
    "     #                       yerr = np.std(lam_results[lam], 0), \n",
    "      #                      ls = ls[str(lam)],  elinewidth=0, ecolor='black', color=(0,0,0,0), lw=1)\n",
    "    #plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam), ls = ls[str(lam)], c='k',\n",
    "     #                  lw=2)\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('hypernets_var.eps', bbox_inches = 'tight')\n",
    "plt.savefig('H_var_log_onestr')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_copy(net, new_net, lam):\n",
    "    lam_param = lam/100.0\n",
    "    for j in range(0, 2): # бежим по слоям        \n",
    "        new_net[j].mean.data*=0\n",
    "        new_net[j].mean.data+=net[j].mean(lam_param)\n",
    "        new_net[j].mean_b.data*=0\n",
    "        new_net[j].mean_b.data+=net[j].mean_b(lam_param)\n",
    "        new_net[j].log_sigma.data*=0\n",
    "        new_net[j].log_sigma.data+=net[j].log_sigma(lam_param)\n",
    "        new_net[j].log_sigma_b.data*=0\n",
    "        new_net[j].log_sigma_b.data+=net[j].log_sigma_b(lam_param)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet = VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x))\n",
    "hnet = hnet.to(device)\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "net = net.to(device)\n",
    "    \n",
    "lam_results = {}\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        print (lam, s)\n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'lowrank_start_{}.cpk'.format(s))))\n",
    "        \n",
    "        net_copy(hnet, net, lam)\n",
    "        lam_results[lam].append(delete_10(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_lowrank.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_lowrank.json') as inp:\n",
    "        lam_results = json.loads(inp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "    \n",
    "for lam in lamb:\n",
    "    lam = str(lam)\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('H_lowrank_log_onestr')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet = VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x))\n",
    "hnet = hnet.to(device)\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "net = net.to(device)\n",
    "    \n",
    "lam_results = {}\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        print (lam, s)\n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'linear_start_{}.cpk'.format(s))))\n",
    "        \n",
    "        net_copy(hnet, net, lam)\n",
    "        lam_results[lam].append(delete_10(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_linear.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "    \n",
    "for lam in lamb:\n",
    "    #lam = str(lam)\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('H_linear_log2')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet = VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x))\n",
    "hnet = hnet.to(device)\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "net = net.to(device)\n",
    "    \n",
    "lam_results = {}\n",
    "t.manual_seed(0)\n",
    "for lam in lamb:    \n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        \n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'lowrank_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        \n",
    "        optim = t.optim.Adam(net.parameters(), lr=5e-4)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)            \n",
    "        label = 'finetune lowrank, lam:{} '.format(lam)                \n",
    "        train_batches(net, loss_fn, optim, lam, label)        \n",
    "        lam_results[lam].append(delete_10(net))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_lowrank_finetune.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "    \n",
    "for lam in lamb:\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('H_low_log+1')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet = VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x))\n",
    "hnet = hnet.to(device)\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "net = net.to(device)\n",
    "t.manual_seed(0)    \n",
    "lam_results = {}\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'linear_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        \n",
    "        optim = t.optim.Adam(net.parameters(), lr=5e-4)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)            \n",
    "        label = 'finetune linear, lam:{} '.format(lam)                \n",
    "        train_batches(net, loss_fn, optim, lam, label)        \n",
    "        lam_results[lam].append(delete_10(net))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_linear_finetune.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "    \n",
    "for lam in lamb:\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('Hypernet_linear + 1_log')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in lamb:\n",
    "    t.manual_seed(0) \n",
    "    for start in range(start_num):                    \n",
    "            net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "            net = net.to(device)\n",
    "            optim = t.optim.Adam(net.parameters(), lr=5e-4)\n",
    "            loss_fn = nn.CrossEntropyLoss().to(device)                        \n",
    "            label = 'lambda {} '.format(lam)                \n",
    "            train_batches(net, loss_fn, optim, lam, label)\n",
    "            t.save(net.state_dict(), os.path.join(path_to_save, 'var_1e_net_lam_{}_start_{}.cpk'.format(lam, start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_results = {}\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    for s in range(start_num):\n",
    "        print (lam, s)\n",
    "        net.load_state_dict(t.load(os.path.join('saved/', 'var_1e_net_lam_{}_start_{}.cpk'.format(lam, s))))\n",
    "        lam_results[lam].append(delete_10(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_1e.json','w') as out:\n",
    "    out.write(json.dumps(lam_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "    \n",
    "for lam in lamb:\n",
    "    plt.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "    plt.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "plt.legend(loc='lower left')\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.savefig('1e.eps', bbox_inches = 'tight')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating accuracy\n",
    "for mode in ['results_var','results_1e', 'results_lowrank', 'results_linear', 'results_lowrank_finetune', 'results_linear_finetune']:\n",
    "    print ('mode:', mode)\n",
    "    with open(mode+'.json') as inp:\n",
    "        data = json.loads(inp.read())\n",
    "    for lam in lamb:\n",
    "        lam = str(lam)\n",
    "        print (lam, np.mean(data[lam][0]), np.std(data[lam][0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating stability\n",
    "for mode in ['results_var','results_1e', 'results_lowrank', 'results_linear', 'results_lowrank_finetune', 'results_linear_finetune']:\n",
    "    print ('mode:', mode)\n",
    "    with open(mode+'.json') as inp:\n",
    "        data = json.loads(inp.read())\n",
    "        \n",
    "    for lam in lamb:\n",
    "        lam = str(lam)\n",
    "        v_0 = np.array(data[lam])[:,0]\n",
    "        v_last = np.array(data[lam])[:,-1]\n",
    "        \n",
    "        stab = v_0/v_last\n",
    "        print (lam, np.mean(stab), np.std(stab))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('ordinal model')\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)   \n",
    "\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):        \n",
    "        net.load_state_dict(t.load(os.path.join('saved/', 'var_net_lam_{}_start_{}.cpk'.format(lam, s))))                 \n",
    "        t.manual_seed(0)\n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]\n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('1e model')\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)   \n",
    "\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):        \n",
    "        net.load_state_dict(t.load(os.path.join('saved/', 'var_1e_net_lam_{}_start_{}.cpk'.format(lam, s))))                 \n",
    "        t.manual_seed(0)\n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]\n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('lr model')\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "hnet = VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)   \n",
    "\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):     \n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'lowrank_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        t.manual_seed(0)\n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]\n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "print ('lr ft model')\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):     \n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'lowrank_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        t.manual_seed(0)\n",
    "        optim = t.optim.Adam(net.parameters(), lr=5e-4)        \n",
    "        train_batches(net, loss_fn, optim, lam, label)   \n",
    "        \n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]  \n",
    "        \n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('linear model')\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "hnet = VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)   \n",
    "\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):     \n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'linear_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        t.manual_seed(0)\n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]\n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "print ('lr ft model')\n",
    "for lam in lamb:\n",
    "    lam_results[lam] = []\n",
    "    k = []\n",
    "    ll = []\n",
    "    for s in range(start_num):     \n",
    "        hnet.load_state_dict(t.load(os.path.join('saved/', 'linear_start_{}.cpk'.format(s))))        \n",
    "        net_copy(hnet, net, lam)\n",
    "        t.manual_seed(0)\n",
    "        optim = t.optim.Adam(net.parameters(), lr=5e-4)        \n",
    "        train_batches(net, loss_fn, optim, lam, label)   \n",
    "        \n",
    "        k+= [net.KLD().cpu().detach().numpy()]\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            ll += [loss_fn(net(x), y).cpu().detach().numpy()]  \n",
    "        \n",
    "    print (lam, np.mean(ll)*len(train_data) + lam * np.mean(k))\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество оптимизаций параметров\n",
    "# количество батчей * количество эпох * количество параметров, домноженное на количество элементов в lamb\n",
    "net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "p_num = 0\n",
    "for p in hnet.parameters():\n",
    "    size = p.size()\n",
    "    if len(size)==1:\n",
    "        p_num += size[0]\n",
    "    elif len(size) == 2:\n",
    "        p_num += np.prod(size)    \n",
    "p_num_var = p_num\n",
    "print ('ordinal', len(train_loader)*epoch_num*(p_num)*len(lamb))\n",
    "print ('1e', len(train_loader)*p_num * len(lamb))\n",
    "\n",
    "hnet = VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x))\n",
    "p_num = 0\n",
    "for p in hnet.parameters():\n",
    "    size = p.size()\n",
    "    if len(size)==1:\n",
    "        p_num += size[0]\n",
    "    elif len(size) == 2:\n",
    "        p_num += np.prod(size)    \n",
    "    \n",
    "print ('low rank', len(train_loader)*epoch_num*(p_num))\n",
    "print ('low rank finetune', len(train_loader)*epoch_num*(p_num) + len(train_loader)*p_num_var)\n",
    "\n",
    "hnet = VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x))\n",
    "p_num = 0\n",
    "for p in hnet.parameters():\n",
    "    size = p.size()\n",
    "    if len(size)==1:\n",
    "        p_num += size[0]\n",
    "    elif len(size) == 2:\n",
    "        p_num += np.prod(size)    \n",
    "    \n",
    "print ('linear', len(train_loader)*epoch_num*(p_num))\n",
    "print ('linear fine tune', len(train_loader)*epoch_num*(p_num) + len(train_loader)*p_num_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new plots\n",
    "for mode in ['results_var','results_1e', 'results_lowrank', 'results_linear', 'results_lowrank_finetune', 'results_linear_finetune']:\n",
    "    print ('mode:', mode)\n",
    "    with open(mode+'.json') as inp:\n",
    "        data = json.loads(inp.read())\n",
    "    for lam in lamb:\n",
    "        lam = str(lam)\n",
    "        print (lam, np.mean(data[lam][0]), np.std(data[lam][0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 20, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "fig, axs = plt.subplots(2, 3)\n",
    "coords = [(0,0), (1,0), (0,1), (1,1), (0,2), (1,2)]\n",
    "titles = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']\n",
    "ls = {\n",
    "    '0.1':'-',\n",
    "    '1':'--',\n",
    "    '10':':',\n",
    "    '100':'-.'\n",
    "}\n",
    "roc = np.array([0,10,20,30,40,50,60,70,80,90])\n",
    "for id, mode in enumerate(['results_var','results_1e', 'results_lowrank', 'results_lowrank_finetune', 'results_linear', 'results_linear_finetune']):\n",
    "    print ('mode:', mode)\n",
    "    with open(mode+'.json') as inp:\n",
    "        lam_results = json.loads(inp.read())\n",
    "    current_ax = axs[coords[id][0], coords[id][1]]\n",
    "\n",
    "\n",
    "    for lam in lamb:\n",
    "        lam = str(lam)\n",
    "        current_ax.fill_between(proc, np.min(lam_results[lam], 0), np.max(lam_results[lam], 0), alpha=0.2)\n",
    "        current_ax.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam))\n",
    "        \n",
    "        # вызываем plot и errorbar два раза:\n",
    "        # (здесь это оказалось ненужным, но оставил на случай переделок:\n",
    "        # если ошибки разных графиков сильно накладываются, их разносят небольшим сдвигом\n",
    "        # а сами линии средних значений оставляют на месте)\n",
    "       # current_ax.errorbar(proc, np.mean(lam_results[lam], 0), \n",
    "        #                    yerr = np.std(lam_results[lam], 0), \n",
    "         #                   ls = ls[lam],  elinewidth=0)#, ecolor='black', color=(0,0,0,0), lw=1)\n",
    "        #current_ax.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam), ls = ls[lam], c='k',\n",
    "         #              lw=2)\n",
    "        \n",
    "        #current_ax.set_title(titles[id], y=-0.05)\n",
    "        if id in [1,3,5]:\n",
    "            current_ax.text(40, .38, titles[id])\n",
    "        else:\n",
    "            current_ax.text(40, .42, titles[id])\n",
    "        current_ax.set_ylim((0.5, 1.0))\n",
    "        if id != 0 and id != 1:\n",
    "            current_ax.set_yticklabels([])\n",
    "        if id not in [1,3,5]:\n",
    "            current_ax.set_xticklabels([])\n",
    "        if id == 5:\n",
    "            current_ax.legend(bbox_to_anchor=(1.1, 1.3))\n",
    "fig.add_subplot(111, frame_on=False)\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "\n",
    "plt.ylabel('Accuracy', fontsize = 27)\n",
    "plt.xlabel('Percentage of removed parameters', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "ax = plt.gca() \n",
    "ax.xaxis.set_label_coords(0.5, -0.15) \n",
    "#plt.autoscale(enable=True, axis='x', tight=False)\n",
    "#fig.tight_layout()\n",
    "plt.savefig('hypernets_colour_begin.png', bbox_inches = 'tight')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 20, 12\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rc('lines', linewidth=4)\n",
    "    \n",
    "fig, axs = plt.subplots(2, 3)\n",
    "coords = [(0,0), (1,0), (0,1), (1,1), (0,2), (1,2)]\n",
    "titles = ['(а)', '(б)', '(в)', '(г)', '(д)', '(е)']\n",
    "ls = {\n",
    "    '0.1':'-',\n",
    "    '1':'--',\n",
    "    '10':':',\n",
    "    '100':'-.'\n",
    "}\n",
    "roc = np.array([0,10,20,30,40,50,60,70,80,90])\n",
    "for id, mode in enumerate(['results_var','results_1e', 'results_lowrank', 'results_lowrank_finetune', 'results_linear', 'results_linear_finetune']):\n",
    "    print ('mode:', mode)\n",
    "    with open(mode+'.json') as inp:\n",
    "        lam_results = json.loads(inp.read())\n",
    "    current_ax = axs[coords[id][0], coords[id][1]]\n",
    "\n",
    "    for lam in lamb:\n",
    "        lam = str(lam)\n",
    "        \n",
    "        # вызываем plot и errorbar два раза:\n",
    "        # (здесь это оказалось ненужным, но оставил на случай переделок:\n",
    "        # если ошибки разных графиков сильно накладываются, их разносят небольшим сдвигом\n",
    "        # а сами линии средних значений оставляют на месте)\n",
    "        current_ax.errorbar(proc, np.mean(lam_results[lam], 0), \n",
    "                            yerr = np.std(lam_results[lam], 0), \n",
    "                            ls = ls[lam],  elinewidth=0, ecolor='black', color=(0,0,0,0), lw=1)\n",
    "        current_ax.plot(proc, np.mean(lam_results[lam], 0), label='$\\lambda={}$'.format(lam), ls = ls[lam], c='k',\n",
    "                       lw=2)\n",
    "        \n",
    "        #current_ax.set_title(titles[id], y=-0.05)\n",
    "        if id in [1,3,5]:\n",
    "            current_ax.text(40, .38, titles[id])\n",
    "        else:\n",
    "            current_ax.text(40, .42, titles[id])\n",
    "        current_ax.set_ylim((0.5, 1.0))\n",
    "        if id != 0 and id != 1:\n",
    "            current_ax.set_yticklabels([])\n",
    "        if id not in [1,3,5]:\n",
    "            current_ax.set_xticklabels([])\n",
    "        if id == 5:\n",
    "            current_ax.legend(bbox_to_anchor=(1.1, 1.3))\n",
    "fig.add_subplot(111, frame_on=False)\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "\n",
    "plt.ylabel('Точность классификации', fontsize = 27)\n",
    "plt.xlabel('Процент удаления параметров', fontsize = 27)\n",
    "plt.tick_params(axis='both', which='major', labelsize=27)\n",
    "ax = plt.gca() \n",
    "ax.xaxis.set_label_coords(0.5, -0.15) \n",
    "#plt.autoscale(enable=True, axis='x', tight=False)\n",
    "#fig.tight_layout()\n",
    "plt.savefig('hypernets.eps', bbox_inches = 'tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(nn.Module): #однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = nn.Parameter(t.randn(in_, out_, device=device)) # параметры средних\n",
    "        t.nn.init.xavier_uniform(self.mean) \n",
    "        self.log_sigma = nn.Parameter(t.ones(in_, out_, device = device)*init_log_sigma) # логарифм дисперсии\n",
    "        self.mean_b = nn.Parameter(t.randn(out_, device=device)) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = nn.Parameter(t.ones(out_, device=device) * init_log_sigma)\n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x):    \n",
    "        w = self.mean \n",
    "        b = self.mean_b\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b,  t.exp(self.log_sigma_b))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
