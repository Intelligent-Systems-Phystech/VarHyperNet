{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from primary_net import PrimaryNetwork\n",
    "\n",
    "from torchvision import datasets\n",
    "import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # cuda or cpu\n",
    "device = t.device(device)\n",
    "if device == 'cuda':\n",
    "    t.backends.cudnn.deterministic = True\n",
    "    t.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "prior_sigma = 1.0 # априорная дисперсия\n",
    "epoch_num = 25 #количество эпох\n",
    "lamb = [0.01, 0.1, 1,  10, 100]\n",
    "start_num = 5\n",
    "\n",
    "lambda_encode = t.log\n",
    "lambda_sample_num = 5\n",
    "path_to_save = 'saved_mnist_lr'\n",
    "\n",
    "if not os.path.exists(path_to_save):\n",
    "    os.mkdir(path_to_save)\n",
    "    \n",
    "learning_rate = 0.002\n",
    "weight_decay = 0.0005\n",
    "milestones = [168000, 336000, 400000, 450000, 550000, 600000]\n",
    "max_iter = 1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = t.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(net, loss_fn, optimizer, lam, label):\n",
    "    tq = tqdm.tqdm(trainloader)\n",
    "    losses = []\n",
    "    for x,y in tq:            \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)          \n",
    "        optimizer.zero_grad()  \n",
    "        loss = 0\n",
    "        if lam is None:\n",
    "            for _ in range(lambda_sample_num):  \n",
    "                p = t.rand(1).to(device)*4 -7\n",
    "                lam_param = 10**p[0]                \n",
    "                #t.rand(1).to(device)[0]*100.0                  \n",
    "                out = net(x, lambda_encode(lam_param))\n",
    "                loss = loss + loss_fn(out, y)/lambda_sample_num\n",
    "                loss += net.KLD(lam_param)*t.log(lam_param)/len(trainset)/lambda_sample_num\n",
    "                losses+=[loss.cpu().detach().numpy()]\n",
    "        tq.set_description(label+str(np.mean(losses)))\n",
    "        loss.backward()       \n",
    "        clip_grad_value_(net.parameters(), 1.0) # для стабильности градиента. С этим можно играться\n",
    "        optimizer.step()\n",
    "        #lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR, epoch 0: 1.3615959: 100%|██████████| 391/391 [21:52<00:00,  3.36s/it]\n",
      "CIFAR, epoch 1: 0.58283424: 100%|██████████| 391/391 [19:30<00:00,  2.99s/it]\n",
      "CIFAR, epoch 2: -0.9957696: 100%|██████████| 391/391 [18:56<00:00,  2.91s/it]  \n",
      "CIFAR, epoch 3: -3.3413982: 100%|██████████| 391/391 [27:57<00:00,  4.29s/it]\n",
      "CIFAR, epoch 4: -6.6035476: 100%|██████████| 391/391 [18:57<00:00,  2.91s/it]\n",
      "CIFAR, epoch 5: -10.59768: 100%|██████████| 391/391 [18:54<00:00,  2.90s/it]  \n",
      "CIFAR, epoch 6: -15.446829: 100%|██████████| 391/391 [18:46<00:00,  2.88s/it] \n",
      "CIFAR, epoch 7: -20.676786: 100%|██████████| 391/391 [19:03<00:00,  2.92s/it]\n",
      "CIFAR, epoch 8: -26.118053: 100%|██████████| 391/391 [17:49<00:00,  2.74s/it]\n",
      "CIFAR, epoch 9: -32.291473: 100%|██████████| 391/391 [18:04<00:00,  2.77s/it]\n",
      "CIFAR, epoch 10: -39.13133: 100%|██████████| 391/391 [18:12<00:00,  2.79s/it] \n",
      "CIFAR, epoch 11: -47.929573: 100%|██████████| 391/391 [18:32<00:00,  2.84s/it]\n",
      "CIFAR, epoch 12: nan: 100%|██████████| 391/391 [17:44<00:00,  2.72s/it]       \n",
      "CIFAR, epoch 13: nan: 100%|██████████| 391/391 [17:14<00:00,  2.65s/it]\n",
      "CIFAR, epoch 14: nan: 100%|██████████| 391/391 [17:10<00:00,  2.64s/it]\n",
      "CIFAR, epoch 15: nan: 100%|██████████| 391/391 [16:50<00:00,  2.59s/it]\n",
      "CIFAR, epoch 16: nan: 100%|██████████| 391/391 [16:57<00:00,  2.60s/it]\n",
      "CIFAR, epoch 17: nan: 100%|██████████| 391/391 [16:55<00:00,  2.60s/it]\n",
      "CIFAR, epoch 18: nan: 100%|██████████| 391/391 [16:56<00:00,  2.60s/it]\n",
      "CIFAR, epoch 19: nan: 100%|██████████| 391/391 [16:45<00:00,  2.57s/it]\n",
      "CIFAR, epoch 20: nan: 100%|██████████| 391/391 [16:48<00:00,  2.58s/it]\n",
      "CIFAR, epoch 21: nan: 100%|██████████| 391/391 [16:46<00:00,  2.57s/it]\n",
      "CIFAR, epoch 22: nan: 100%|██████████| 391/391 [16:44<00:00,  2.57s/it]\n",
      "CIFAR, epoch 23: nan: 100%|██████████| 391/391 [16:43<00:00,  2.57s/it]\n",
      "CIFAR, epoch 24: nan: 100%|██████████| 391/391 [16:46<00:00,  2.57s/it]\n",
      "CIFAR, epoch 0: 1.302822: 100%|██████████| 391/391 [17:54<00:00,  2.75s/it] \n",
      "CIFAR, epoch 1: 0.57665366: 100%|██████████| 391/391 [17:42<00:00,  2.72s/it]\n",
      "CIFAR, epoch 2: -1.0076606: 100%|██████████| 391/391 [17:38<00:00,  2.71s/it]  \n",
      "CIFAR, epoch 3: -3.5696843: 100%|██████████| 391/391 [17:31<00:00,  2.69s/it]\n",
      "CIFAR, epoch 4: -6.6638665: 100%|██████████| 391/391 [17:44<00:00,  2.72s/it]\n",
      "CIFAR, epoch 5: -10.54396: 100%|██████████| 391/391 [17:39<00:00,  2.71s/it]  \n",
      "CIFAR, epoch 6: -14.952567: 100%|██████████| 391/391 [17:30<00:00,  2.69s/it] \n",
      "CIFAR, epoch 7: -19.9579: 100%|██████████| 391/391 [17:27<00:00,  2.68s/it]  \n",
      "CIFAR, epoch 8: -25.231415: 100%|██████████| 391/391 [17:41<00:00,  2.72s/it]\n",
      "CIFAR, epoch 9: -31.015587: 100%|██████████| 391/391 [17:37<00:00,  2.70s/it]\n",
      "CIFAR, epoch 10: -37.16473: 100%|██████████| 391/391 [17:39<00:00,  2.71s/it] \n",
      "CIFAR, epoch 11: -43.88028: 100%|██████████| 391/391 [17:49<00:00,  2.74s/it] \n",
      "CIFAR, epoch 12: -50.513878: 100%|██████████| 391/391 [17:31<00:00,  2.69s/it]\n",
      "CIFAR, epoch 13: -59.611244: 100%|██████████| 391/391 [17:34<00:00,  2.70s/it]\n",
      "CIFAR, epoch 14: -69.289345: 100%|██████████| 391/391 [17:30<00:00,  2.69s/it]\n",
      "CIFAR, epoch 15: nan: 100%|██████████| 391/391 [17:00<00:00,  2.61s/it]       \n",
      "CIFAR, epoch 16: nan: 100%|██████████| 391/391 [16:51<00:00,  2.59s/it]\n",
      "CIFAR, epoch 17: nan: 100%|██████████| 391/391 [17:03<00:00,  2.62s/it]\n",
      "CIFAR, epoch 18: nan: 100%|██████████| 391/391 [17:04<00:00,  2.62s/it]\n",
      "CIFAR, epoch 19: nan: 100%|██████████| 391/391 [17:04<00:00,  2.62s/it]\n",
      "CIFAR, epoch 20: nan: 100%|██████████| 391/391 [17:12<00:00,  2.64s/it]\n",
      "CIFAR, epoch 21: nan: 100%|██████████| 391/391 [17:11<00:00,  2.64s/it]\n",
      "CIFAR, epoch 22: nan: 100%|██████████| 391/391 [17:34<00:00,  2.70s/it]\n",
      "CIFAR, epoch 23: nan: 100%|██████████| 391/391 [17:27<00:00,  2.68s/it]\n",
      "CIFAR, epoch 24: nan: 100%|██████████| 391/391 [17:20<00:00,  2.66s/it]\n",
      "CIFAR, epoch 0: 1.3331: 100%|██████████| 391/391 [17:56<00:00,  2.75s/it]   \n",
      "CIFAR, epoch 1: 0.6089166: 100%|██████████| 391/391 [18:12<00:00,  2.79s/it] \n",
      "CIFAR, epoch 2: -0.945524: 100%|██████████| 391/391 [18:05<00:00,  2.78s/it]  \n",
      "CIFAR, epoch 3: -3.3434682: 100%|██████████| 391/391 [17:56<00:00,  2.75s/it]\n",
      "CIFAR, epoch 4: -6.586468: 100%|██████████| 391/391 [17:59<00:00,  2.76s/it] \n",
      "CIFAR, epoch 5: -10.581561: 100%|██████████| 391/391 [17:53<00:00,  2.74s/it] \n",
      "CIFAR, epoch 6: -14.954272: 100%|██████████| 391/391 [17:56<00:00,  2.75s/it] \n",
      "CIFAR, epoch 7: -20.025963: 100%|██████████| 391/391 [17:46<00:00,  2.73s/it]\n",
      "CIFAR, epoch 8: -25.128025: 100%|██████████| 391/391 [17:50<00:00,  2.74s/it]\n",
      "CIFAR, epoch 9: -31.409819: 100%|██████████| 391/391 [17:36<00:00,  2.70s/it]\n",
      "CIFAR, epoch 10: -37.344074: 100%|██████████| 391/391 [17:53<00:00,  2.74s/it]\n",
      "CIFAR, epoch 11: -44.032906: 100%|██████████| 391/391 [17:55<00:00,  2.75s/it]\n",
      "CIFAR, epoch 12: -51.84931: 100%|██████████| 391/391 [18:12<00:00,  2.80s/it] \n",
      "CIFAR, epoch 13: -59.9174: 100%|██████████| 391/391 [18:02<00:00,  2.77s/it]  \n",
      "CIFAR, epoch 14: -69.069214: 100%|██████████| 391/391 [17:50<00:00,  2.74s/it]\n",
      "CIFAR, epoch 15: -81.7836: 100%|██████████| 391/391 [17:43<00:00,  2.72s/it]  \n",
      "CIFAR, epoch 16: nan: 100%|██████████| 391/391 [17:13<00:00,  2.64s/it]       \n",
      "CIFAR, epoch 17: nan: 100%|██████████| 391/391 [16:43<00:00,  2.57s/it]\n",
      "CIFAR, epoch 18: nan: 100%|██████████| 391/391 [16:46<00:00,  2.57s/it]\n",
      "CIFAR, epoch 19: nan: 100%|██████████| 391/391 [16:48<00:00,  2.58s/it]\n",
      "CIFAR, epoch 20: nan: 100%|██████████| 391/391 [16:34<00:00,  2.54s/it]\n",
      "CIFAR, epoch 21: nan: 100%|██████████| 391/391 [16:43<00:00,  2.57s/it]\n",
      "CIFAR, epoch 22: nan: 100%|██████████| 391/391 [16:41<00:00,  2.56s/it]\n",
      "CIFAR, epoch 23: nan: 100%|██████████| 391/391 [16:45<00:00,  2.57s/it]\n",
      "CIFAR, epoch 24: nan: 100%|██████████| 391/391 [16:41<00:00,  2.56s/it]\n",
      "CIFAR, epoch 0: 1.3389609: 100%|██████████| 391/391 [17:41<00:00,  2.71s/it]\n",
      "CIFAR, epoch 1: 0.6782385: 100%|██████████| 391/391 [17:32<00:00,  2.69s/it] \n",
      "CIFAR, epoch 2: -0.8239395: 100%|██████████| 391/391 [17:41<00:00,  2.71s/it]  \n",
      "CIFAR, epoch 3: -3.2614381: 100%|██████████| 391/391 [17:36<00:00,  2.70s/it]\n",
      "CIFAR, epoch 4: -6.431243: 100%|██████████| 391/391 [17:34<00:00,  2.70s/it] \n",
      "CIFAR, epoch 5: -10.249699: 100%|██████████| 391/391 [17:45<00:00,  2.72s/it] \n",
      "CIFAR, epoch 6: -14.693298: 100%|██████████| 391/391 [17:29<00:00,  2.69s/it] \n",
      "CIFAR, epoch 7: -19.791553: 100%|██████████| 391/391 [17:23<00:00,  2.67s/it]\n",
      "CIFAR, epoch 8: -25.062025: 100%|██████████| 391/391 [17:30<00:00,  2.69s/it]\n",
      "CIFAR, epoch 9: -30.522467: 100%|██████████| 391/391 [17:21<00:00,  2.66s/it]\n",
      "CIFAR, epoch 10: -34.140087:  11%|█         | 42/391 [01:50<14:39,  2.52s/it]"
     ]
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "for start in range(start_num):         \n",
    "    net = PrimaryNetwork(prior_sigma = prior_sigma, device = device)\n",
    "    net = net.to(device)\n",
    "    optim = t.optim.Adam(net.parameters(), lr=1e-4)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.5)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)            \n",
    "    for e in range(epoch_num):\n",
    "        label = 'CIFAR, epoch {}: '.format(e)                \n",
    "        train_batches(net, loss_fn, optim, None, label)\n",
    "    t.save(net.state_dict(), os.path.join(path_to_save, 'cifar_start_{}.cpk'.format( start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
